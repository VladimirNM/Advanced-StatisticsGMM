{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFagjtvCWC14"
      },
      "source": [
        "**Task on estimation of parameters for Gamma distribution (done by Vladimir Manaev).**\n",
        "24th of October 2021.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BupsZu9sV_UL"
      },
      "source": [
        "# load important libraries and data\n",
        "import numpy as np\n",
        "import pickle\n",
        "import scipy\n",
        "X = np.array([20.5, 31.5, 47.7, 26.2, 44.0, 8.28, 30.8, 17.2, 19.9, 9.96, 55.8, 25.2, 29.0, 85.5, 15.1, 28.5, 21.4, 17.7, 6.42, 84.9])\n",
        "n = len(X)\n",
        "X_mean = X.mean()\n",
        "X_sum = X.sum()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1RqE9LEWUbj"
      },
      "source": [
        "#Part 1: MLE estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKR74CZAXKSQ"
      },
      "source": [
        "To estimate maximum likelihood parameters we have to do the following steps:\n",
        "1. Write the likelihood function\n",
        "2. Take a natural log\n",
        "3. MLE estimates maximize the likelihood function, so we can take F.O.Cs\n",
        "4. First order conditions in our case will be represented by the folloing system:\n",
        "\n",
        "$\\psi$($\\alpha$)=ln($\\beta$)+(1/n)*$\\sum_{i=1}^{n} ln(X_{i})$\n",
        "\n",
        "$\\alpha$=$\\beta$* (1/n)*$\\sum_{i=1}^{n} X_{i}$\n",
        "\n",
        "where $\\psi$($\\alpha$): digamma function\n",
        "\n",
        "Numerical solution of the system will give us the maximum likelihood estimation of the parameters: $\\alpha_{MLE}$, $\\beta_{MLE}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnNsmO_ZdfR3",
        "outputId": "4bbf2615-96a6-4944-b63f-c1f5711da44f"
      },
      "source": [
        "#import optimizer and digamma function\n",
        "from scipy.special import digamma\n",
        "from scipy.optimize import fsolve\n",
        "import math\n",
        "\n",
        "# here define the system\n",
        "def equations(p):\n",
        "    alpha, beta = p\n",
        "    return (digamma(alpha) - np.log(beta) - 1/n * np.sum([np.log(x) for x in X]) , alpha - beta * np.mean(X))\n",
        "\n",
        "# here check that numerical solutions actually solve each of the equation\n",
        "def eq1(alpha, beta):\n",
        "  return digamma(alpha) - np.log(beta) - 1/n * np.sum([np.log(x) for x in X])\n",
        "\n",
        "def eq2(alpha, beta):\n",
        "  return alpha - beta * np.mean(X)\n",
        "\n",
        "alpha, beta =  fsolve(equations, (1, 1))\n",
        "\n",
        "print(eq1(alpha, beta))\n",
        "print(eq2(alpha, beta))\n",
        "# here check what are the estimate values\n",
        "print(alpha, beta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "0.0\n",
            "2.410601621059194 0.07707019697740246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS5lZOqRgC4k"
      },
      "source": [
        "Once we get the estimates we need to study the asymptotic variance of the estimators.\n",
        "We have theorem that states:\n",
        "1. MLE estimates are consistent (tend to true value as n goes to infinity)\n",
        "2. Asymptotic variance will be estimated as the inverse of Fisher information matrix estimated at MLE values and averaged over n.\n",
        "\n",
        "Fisher information matrix will be minus expected value of Hessian of loglikelihood:\n",
        "\n",
        "I($\\alpha$,$\\beta$)=-E[H]\n",
        "\n",
        "Where H defined as:\n",
        "\n",
        "\\begin{pmatrix}\n",
        "-trigamma(\\alpha) & 1/\\beta\\\\\n",
        "1/\\beta & \\alpha/\\beta^2\n",
        "\\end{pmatrix}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHTCpslBfF61",
        "outputId": "395d02e1-daee-4f98-aa32-7e85b63eec90"
      },
      "source": [
        "tri_alpha = scipy.special.polygamma(1,alpha)\n",
        "\n",
        "h1 = np.array(\n",
        "    [[tri_alpha, - 1/beta],\n",
        "     [-1/beta, alpha/beta**2]]\n",
        "    )\n",
        "Fisher=(1/n)*np.linalg.inv(h1)\n",
        "Fisher\n",
        "std_of_alpha=np.sqrt(0.51243413)\n",
        "std_of_beta=np.sqrt(0.000647)\n",
        "print(std_of_alpha,std_of_beta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7158450460819018 0.02543619468395381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0898tqQnaLE"
      },
      "source": [
        "Our estimates of asymptotic MLE parameters standard deviations:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzZAM7CUlwZw",
        "outputId": "f9f8c162-e8b4-43c6-b348-0c9a30af9556"
      },
      "source": [
        "print(std_of_alpha,std_of_beta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7158450460819018 0.02543619468395381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpnRO-vooMzi"
      },
      "source": [
        "#Part 2: Generalized method of moments for Gamma distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cKgkZeloXgK"
      },
      "source": [
        "For GMM estimation, we need to equate the given theoretical moments of Gamma distribution to their sample analogies:\n",
        "\n",
        "E[X] = $\\alpha$/$\\beta$ = (1/n)*$\\sum_{i=1}^{n} X_{i}$\n",
        "\n",
        "E[X^2]=$\\alpha$*($\\alpha$+1)/$\\beta$^2=(1/n)* $\\sum_{i=1}^{n} X_{i}^2$\n",
        "\n",
        "E[1/X]=$\\beta$/($\\alpha$-1) = (1/n)*$\\sum_{i=1}^{n} 1/X_{i}$\n",
        "\n",
        "E[lnX]=$\\psi$($\\alpha$)-ln($\\beta$)=(1/n)*$\\sum_{i=1}^{n} lnX_{i}$\n",
        "\n",
        "One can observe that the system is overidentified: four equations versus two unknowns.\n",
        "\n",
        "So, the traditional method of moments (MM) will not work here. For traditional MM we have two choose 2 equations out of 4.\n",
        "\n",
        "Generalized method of moments might be good alternative here, if we specify coherent penalty matrix.\n",
        "\n",
        "The idea of GMM with equal (unity) costs that we optimize cost function with respect to two parameters: alpha and beta.\n",
        "\n",
        "In other words, we choose alpha and beta in such way, so the total squared distance is minimal.\n",
        "\n",
        "Squared_cost_fun($\\alpha$,$\\beta$)=(m1-$\\alpha$/$\\beta$)^2+(m2-$\\alpha$*($\\alpha$+1)/$\\beta$^2)^2+(m3-$\\beta$/($\\alpha$-1))^2+(m4-$\\psi$($\\alpha$)+ln($\\beta$))^2\n",
        "\n",
        "where m1,m2,m3,m4 are sample estimates of the moments.\n",
        "\n",
        "So, the solution to the minimization of cost function will give us:\n",
        " $\\alpha_{GMM}$, $\\beta_{GMM}$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bitp3hDLu7BM"
      },
      "source": [
        "# Estimation of sample moments\n",
        "m1 = X.mean()\n",
        "m2 = np.sum([x*x for x in X])/n\n",
        "m3 = np.sum([1/x for x in X])/n\n",
        "m4 = np.sum([np.log(x) for x in X])/n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjaXasfLvMbn"
      },
      "source": [
        "#Define squared cost function\n",
        "def fun(x0):\n",
        "  alpha_m = x0[0]\n",
        "  beta_m = x0[1]\n",
        "  g1 = m1 - alpha_m/beta_m\n",
        "  g2 = m2 - alpha_m*(alpha_m + 1)/beta_m**2\n",
        "  g3 = m3 - beta_m/(alpha_m - 1)\n",
        "  g4 = m4 - digamma(alpha_m) + np.log(beta_m)\n",
        "  return g1 ** 2 +  g2 ** 2 +   g3 **2+ g4 **2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5VAauBhv4vH",
        "outputId": "3a9ba947-50c2-4e4b-e042-a31ba6e513f1"
      },
      "source": [
        "#Optimize the function\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "x0 = [1,1]\n",
        "res = minimize(fun, x0, method='nelder-mead',\n",
        "               options={'xatol': 1e-8, 'disp': True})\n",
        "alpha_m, beta_m = res.x\n",
        "alpha_m, beta_m"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.001803\n",
            "         Iterations: 115\n",
            "         Function evaluations: 223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in log\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.0582996665314375, 0.0657988790734718)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKga2XWZwqL5"
      },
      "source": [
        "Theorem of consistency of GMM estimators states that our estimates will be consistent as well under some mild technical conditions. \n",
        "\n",
        "So, GMM with unity penalties provides us the following estimates:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZI4T1LUw6HO",
        "outputId": "b4b6b9d7-e1c8-4a85-9cc3-567c64ff3b03"
      },
      "source": [
        "print(alpha_m,beta_m)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0582996665314375 0.0657988790734718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rze64waWxQgm"
      },
      "source": [
        "One can easily check that the optimization above is equivalent to optimization of quadratic form:\n",
        "gWg_transpose\n",
        "\n",
        "where \n",
        "\n",
        "g is the vector of non-squared cost functions\n",
        "\n",
        "W=diag(1) square matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIIf76lvyJB9"
      },
      "source": [
        "If we want to achieve efficiency here, we need to pick the weight matrix in a \"smart\" way, so it is proportional to inverse of variances."
      ]
    }
  ]
}